# AI Proctoring System - DeepFace Optimized

A modular proctoring system with **MediaPipe face detection** and **DeepFace face matching** with configurable backends. Optimized to pass pre-cropped faces directly to DeepFace, eliminating redundant face detection for **2x performance improvement**.

## üöÄ Key Features

- **MediaPipe Face Detection**: Fast, accurate face detection with 468-point mesh
- **DeepFace Integration**: 8 configurable backends for face matching
- **Pre-Cropped Face Optimization**: Eliminates redundant detection (~2x faster)
- **Session Logging**: Comprehensive JSON/text logging for audit trails
- **Real-time Verification**: Threaded face matching with visual feedback
- **Modular Architecture**: Easy to extend and customize
- **Multi-Face Alert System**: Detects and alerts on multiple faces
- **Configurable Thresholds**: Fine-tune matching sensitivity per backend

## üéØ Optimization Highlight

**The system passes pre-cropped faces from MediaPipe directly to DeepFace's embedding extraction, bypassing DeepFace's internal detection using `enforce_detection=False`. This eliminates redundant face detection, achieving ~2x performance improvement.**

## üìã Project Structure

```
HD_ML_stuff/
‚îú‚îÄ‚îÄ proctor_main.py                          # Main proctoring application
‚îú‚îÄ‚îÄ example_deepface_integration.py          # DeepFace backend demo
‚îú‚îÄ‚îÄ requirements.txt                         # Python dependencies
‚îú‚îÄ‚îÄ README.md                                # This file
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                         # Module exports
‚îÇ   ‚îú‚îÄ‚îÄ base_detector.py                    # Base detector class
‚îÇ   ‚îú‚îÄ‚îÄ camera_input.py                     # Camera capture module
‚îÇ   ‚îú‚îÄ‚îÄ camera_pipeline.py                  # Base pipeline class
‚îÇ   ‚îú‚îÄ‚îÄ config.py                           # Configuration settings
‚îÇ   ‚îú‚îÄ‚îÄ display.py                          # Display window module
‚îÇ   ‚îú‚îÄ‚îÄ face_detector.py                    # MediaPipe face detection
‚îÇ   ‚îú‚îÄ‚îÄ face_matcher.py                     # DeepFace face matching ‚≠ê
‚îÇ   ‚îú‚îÄ‚îÄ proctor_logger.py                   # Session logging
‚îÇ   ‚îî‚îÄ‚îÄ proctor_pipeline.py                 # Proctoring orchestration
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ participant.png                     # Reference participant image
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îî‚îÄ‚îÄ proctoring/                         # Session logs directory
‚îî‚îÄ‚îÄ docs/
    ‚îú‚îÄ‚îÄ ARCHITECTURE.md                      # System architecture
    ‚îú‚îÄ‚îÄ DEEPFACE_BACKENDS.md                 # Backend configuration guide ‚≠ê
    ‚îú‚îÄ‚îÄ PROCTORING_README.md                 # Proctoring documentation
    ‚îî‚îÄ‚îÄ QUICKSTART.md                        # Quick start guide
```

## üé® Available DeepFace Backends

| Backend | Accuracy | Speed | Embedding Size | Threshold (cosine) | Best For |
|---------|----------|-------|----------------|-------------------|----------|
| **Facenet512** ‚≠ê | Very High | Fast | 512 | 0.30 | Production (recommended) |
| **VGG-Face** | High | Moderate | 2622 | 0.40 | High accuracy |
| **Facenet** | High | Very Fast | 128 | 0.40 | Real-time |
| **ArcFace** | State-of-art | Moderate | 512 | 0.68 | Maximum accuracy |
| **OpenFace** | Moderate | Very Fast | 128 | 0.10 | Lightweight |
| **Dlib** | High | Fast | 128 | 0.07 | Traditional CV |
| **SFace** | Moderate-High | Fast | 128 | 0.593 | Balanced |
| **DeepFace** | High | Slow | 4096 | 0.23 | Research only |

**See [docs/DEEPFACE_BACKENDS.md](docs/DEEPFACE_BACKENDS.md) for detailed backend configuration guide.**

## üèóÔ∏è System Architecture

### Detection Flow

```
Camera ‚Üí MediaPipe Detection ‚Üí Pre-Crop Face ‚Üí DeepFace Embedding ‚Üí Compare ‚Üí Alert
                                    ‚Üì
                             (Skip redundant
                              face detection)
```

## Installation & Setup

### Prerequisites

- Python 3.8 or higher
- Webcam or camera device
- Virtual environment (recommended)

### Step 1: Clone and Navigate

```bash
cd HD_ML_stuff
```

### Step 2: Create Virtual Environment (Recommended)

```bash
# Windows
python -m venv .venv
.venv\Scripts\activate

# Linux/Mac
python3 -m venv .venv
source .venv/bin/activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

**Dependencies include:**
- `opencv-python` - Camera and image processing
- `mediapipe` - Face detection
- `deepface` - Face matching with multiple backends
- `tensorflow` - DeepFace backend models

**Note:** First run will download DeepFace model weights (~100-500MB depending on backend)

### Step 4: Download MediaPipe Face Landmarker Model

The system requires the MediaPipe Face Landmarker model for face detection. Download it from the official MediaPipe website:

**Download Link:** [https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker/index#models](https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker/index#models)

1. Navigate to the models section on the page
2. Download `face_landmarker.task` (recommended: face_landmarker_v2_with_blendshapes.task)
3. Place the downloaded file in the `cv_models/` directory:
   ```
   cv_models/face_landmarker.task
   ```

**Model Options:**
- **face_landmarker.task** - Standard model (~5MB)
- **face_landmarker_v2_with_blendshapes.task** - Enhanced with blendshapes (~5MB)

**Note:** The model path is configured in `modules/config.py` as `FACE_MARKER_MODEL_PATH = "cv_models/face_landmarker.task"`

### Step 6: Add Participant Reference Image

Place a clear photo of the participant at:
```
data/participant.png
```

**Image Requirements:**
- Clear, frontal face
- Good lighting
- High resolution (recommended: 512x512 or higher)
- Single face in image

## üöÄ Quick Start

### Basic Usage

```bash
# Run the proctoring system with default settings
python proctor_main.py
```

### Try Different Backends

```bash
# Run interactive backend demo
python example_deepface_integration.py
```

This will let you test different DeepFace backends (Facenet512, VGG-Face, ArcFace) in real-time.

## ‚öôÔ∏è Configuration

Edit `modules/config.py` to customize settings:

```python
# Face Detection (MediaPipe)
FACE_DETECTION_ENABLED = True
FACE_MODEL_SELECTION = 1  # 0=short-range, 1=full-range
FACE_MIN_DETECTION_CONFIDENCE = 0.7

# Face Matching (DeepFace)
FACE_MATCHING_ENABLED = True
FACE_MATCHING_BACKEND = "Facenet512"  # See backend table above
FACE_MATCHING_DISTANCE_METRIC = "cosine"
FACE_MATCHING_THRESHOLD = 0.30  # Lower = stricter

# Performance
FRAME_SKIP = 2  # Process every 3rd frame
CAMERA_FPS = 30
CAMERA_WIDTH = 1080
CAMERA_HEIGHT = 720

# Logging
PROCTORING_LOG_DIR = "logs/proctoring"
SAVE_SESSION_LOGS = True
```

## üí° Usage Examples

### Example 1: Basic Proctoring

```python
from modules import ProctorPipeline, FaceDetector, FaceMatcher, Config

# Configure backend
Config.FACE_MATCHING_BACKEND = "Facenet512"
Config.FACE_MATCHING_THRESHOLD = 0.30

# Create pipeline
proctor = ProctorPipeline(config=Config)

# Register detectors
face_detector = FaceDetector(enabled=True)
face_matcher = FaceMatcher(
    model_name=Config.FACE_MATCHING_BACKEND,
    distance_threshold=Config.FACE_MATCHING_THRESHOLD
)

proctor.register_detector(face_detector)
proctor.register_detector(face_matcher)

# Start proctoring
proctor.start()
```

### Example 2: Custom Backend Configuration

```python
from modules import FaceMatcher, Config

# Use ArcFace for maximum accuracy
Config.FACE_MATCHING_BACKEND = "ArcFace"
Config.FACE_MATCHING_THRESHOLD = 0.68

face_matcher = FaceMatcher(
    model_name="ArcFace",
    distance_metric="cosine",
    distance_threshold=0.68,
    participant_image_path="data/participant.png"
)

face_matcher.load_model()

# Match a pre-cropped face
result = face_matcher.match_with_details(face_roi)

if result['matched']:
    print(f"‚úì VERIFIED - Distance: {result['distance']:.4f}")
else:
    print(f"‚úó UNVERIFIED - Distance: {result['distance']:.4f}")
```

### Example 3: Pre-Cropped Face Optimization

```python
# This is the KEY optimization - pass pre-cropped faces

# Step 1: Detect face with MediaPipe
face_meshes = face_detector.detect(frame)

# Step 2: Extract pre-cropped face ROI
bbox = face_meshes[0]['bbox']
x, y, w, h = bbox['x'], bbox['y'], bbox['w'], bbox['h']
face_roi = frame[y:y+h, x:x+w]  # Pre-cropped face

# Step 3: Match without redundant detection
# DeepFace skips detection and goes straight to embedding extraction
result = face_matcher.match_with_details(face_roi)

# Result: ~2x faster than full DeepFace pipeline
```

## üìä Performance Benchmarks

**Testing Environment:**
- CPU: Intel i7-10750H
- Resolution: 1080x720
- Backend: Facenet512

| Pipeline | Detection Time | Matching Time | Total | FPS |
|----------|---------------|---------------|-------|-----|
| **Optimized (Pre-cropped)** | ~15ms | ~25ms | ~40ms | 25 |
| Standard DeepFace | ~15ms | ~50ms | ~65ms | 15 |

**Performance Gain: ~37% faster, ~67% more FPS**

## üîß Troubleshooting

### DeepFace Import Error

```bash
# Install DeepFace and TensorFlow
pip install deepface tensorflow

# For GPU support (optional)
pip install tensorflow-gpu
```

### Model Download Issues

Models are downloaded on first run to `~/.deepface/weights/`. Ensure:
- Internet connection available
- Sufficient disk space (~500MB for all models)
- No firewall blocking downloads

### Performance Issues

1. **Use faster backend**: Facenet, OpenFace, Dlib
2. **Increase frame skip**: `Config.FRAME_SKIP = 3` (every 4th frame)
3. **Reduce resolution**: `Config.CAMERA_WIDTH = 720`
4. **Enable GPU**: Install tensorflow-gpu

### False Positives/Negatives

1. **Lower threshold** for stricter matching
2. **Try different backend** (ArcFace for accuracy, Facenet512 for balance)
3. **Improve reference image** (good lighting, clear face, high resolution)
4. **Check participant image quality**

## üìö Documentation

- [**DEEPFACE_BACKENDS.md**](docs/DEEPFACE_BACKENDS.md) - Comprehensive backend configuration guide
- [**ARCHITECTURE.md**](docs/ARCHITECTURE.md) - System architecture details
- [**PROCTORING_README.md**](docs/PROCTORING_README.md) - Proctoring system documentation
- [**QUICKSTART.md**](docs/QUICKSTART.md) - Quick start guide

## üéì How It Works

### 1. Face Detection (MediaPipe)
```python
# MediaPipe detects face and provides bounding box + 468 landmarks
face_meshes = face_detector.detect(frame)
# Returns: [{'bbox': {x, y, w, h}, 'landmarks': [...], 'confidence': 0.95}]
```

### 2. Pre-Crop Face ROI
```python
# Extract face region from frame (KEY OPTIMIZATION)
bbox = face_meshes[0]['bbox']
face_roi = frame[bbox['y']:bbox['y']+bbox['h'], 
                 bbox['x']:bbox['x']+bbox['w']]
```

### 3. Extract Embedding (DeepFace)
```python
# DeepFace extracts embedding WITHOUT face detection
embedding = DeepFace.represent(
    img_path=face_roi,
    model_name="Facenet512",
    enforce_detection=False,  # Skip detection (already cropped)
    detector_backend='skip'   # Skip backend entirely
)
```

### 4. Compute Distance
```python
# Compare with cached participant embedding
distance = compute_distance(current_embedding, participant_embedding)
is_match = distance < threshold  # Lower distance = more similar
```

### 5. Display Result
```python
if is_match:
    cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 2)  # Green
    cv2.putText(frame, "VERIFIED", (x, y-10), ...)
else:
    cv2.rectangle(frame, (x,y), (x+w,y+h), (0,0,255), 2)  # Red
    cv2.putText(frame, "UNVERIFIED", (x, y-10), ...)
```

## üìù Session Logging

The system automatically logs all proctoring events:

```
logs/proctoring/
‚îú‚îÄ‚îÄ session_20260120_143052.log          # Text log
‚îî‚îÄ‚îÄ session_20260120_143052_alerts.json  # JSON alerts
```

**Log Contents:**
- Session start/end times
- Face detection events
- Verification results
- Alert triggers (multiple faces, no face, etc.)
- Performance metrics

## ü§ù Contributing

Contributions welcome! Areas for improvement:
- Additional face detection backends
- More embedding models
- Advanced alert systems
- Performance optimizations
- Documentation improvements

## üìÑ License

This project is for educational and research purposes.

## üôè Acknowledgments

- **MediaPipe** by Google for face detection
- **DeepFace** by Serengil for face recognition
- **OpenCV** for computer vision utilities

---

**Version**: 2.0 (DeepFace Integration)  
**Last Updated**: January 2026

```bash
# Create virtual environment
python -m venv .venv

# Activate on Windows
.venv\Scripts\activate

# Activate on Linux/Mac
source .venv/bin/activate
```

### Step 2: Install Dependencies

```bash
pip install -r requirements.txt
```

**Required packages:**
- `opencv-python` - Camera capture and image processing
- `numpy` - Array operations
- `ultralytics` - YOLOv8 model
- `torch` - PyTorch backend for YOLOv8
- `torchvision` - Computer vision utilities

### Step 3: Run the Pipeline

```bash
cd camera_pipeline
python main.py
```

**On first run**, YOLOv8 will automatically download the model file (~12MB for yolov8s).

### Step 4: Exit

Press `q` or `ESC` to stop the pipeline.

## Configuration Options

Edit [modules/config.py](modules/config.py) or override in your code:

### Camera Settings

| Setting | Default | Description |
|---------|---------|-------------|
| `CAMERA_ID` | 0 | Camera device ID (0 for default webcam) |
| `CAMERA_WIDTH` | 1080 | Camera frame width in pixels |
| `CAMERA_HEIGHT` | 720 | Camera frame height in pixels |
| `CAMERA_FPS` | 30 | Camera frame rate |

### Display Settings

| Setting | Default | Description |
|---------|---------|-------------|
| `WINDOW_NAME` | "Camera Pipeline" | Display window title |
| `FULLSCREEN` | False | Enable fullscreen mode |
| `SHOW_FPS` | True | Display FPS counter |

### Detection Settings

| Setting | Default | Description |
|---------|---------|-------------|
| `FACE_MODEL_TYPE` | "yolov8" | Detector type ("yolov8" or "haar") |
| `FACE_MODEL_NAME` | "yolov8s.pt" | YOLOv8 model name |
| `FACE_CONFIDENCE_THRESHOLD` | 0.5 | Minimum detection confidence (0.0-1.0) |
| `FACE_TARGET_CLASSES` | [0] | COCO class IDs to detect ([0] = person) |
| `FACE_BOX_COLOR` | (0, 255, 0) | Bounding box color (B, G, R) |
| `FACE_BOX_THICKNESS` | 2 | Bounding box line thickness |
| `FACE_SHOW_CONFIDENCE` | True | Show confidence scores |

### Performance Settings

| Setting | Default | Description |
|---------|---------|-------------|
| `MAX_FPS` | 60 | Maximum FPS to process |
| `FRAME_SKIP` | 0 | Number of frames to skip (0 = no skip) |
| `ENABLE_LOGGING` | True | Enable console logging |
| `LOG_LEVEL` | "INFO" | Logging level |

## Usage Examples

### Basic Usage

Run with default settings (person detection):

```bash
python main.py
```

Press `ESC` or `q` to exit.

### Custom Model Selection

Choose different YOLOv8 models for speed/accuracy tradeoff:

```python
from modules import Config

# Nano model (fastest, less accurate)
Config.FACE_MODEL_NAME = "yolov8n.pt"

# Small model (balanced)
Config.FACE_MODEL_NAME = "yolov8s.pt"

# Medium model (more accurate, slower)
Config.FACE_MODEL_NAME = "yolov8m.pt"

# Large model (most accurate, slowest)
Config.FACE_MODEL_NAME = "yolov8l.pt"
```

### Detect Multiple Object Types

Detect different objects using COCO class IDs:

```python
from modules import Config

# Detect persons only (default)
Config.FACE_TARGET_CLASSES = [0]

# Detect persons and cars
Config.FACE_TARGET_CLASSES = [0, 2]

# Detect all objects
Config.FACE_TARGET_CLASSES = None

# Common COCO class IDs:
# 0: person, 1: bicycle, 2: car, 3: motorcycle
# 16: dog, 17: cat, 39: bottle, 41: cup
```

### Adjust Detection Sensitivity

```python
from modules import Config

# Lower threshold = more detections (may include false positives)
Config.FACE_CONFIDENCE_THRESHOLD = 0.3

# Higher threshold = fewer detections (more accurate)
Config.FACE_CONFIDENCE_THRESHOLD = 0.7

# Customize camera settings
Config.CAMERA_WIDTH = 1280
Config.CAMERA_HEIGHT = 720
Config.SHOW_FPS = True
```

## Extending the Pipeline

### Custom Frame Processing

Create custom pipelines by extending the base class:

```python
from modules import CameraPipeline
import cv2

class CustomPipeline(CameraPipeline):
    def process_frame(self, frame):
        # Your custom processing here
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        return cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)

# Run custom pipeline
pipeline = CustomPipeline()
pipeline.run()
```

See [example_custom_processing.py](example_custom_processing.py) for more examples.

## Troubleshooting

### Camera Not Found

```
Error: Failed to open camera 0
```

**Solution**: Try different camera IDs or check camera permissions
```python
Config.CAMERA_ID = 1  # Try different IDs: 0, 1, 2...
```

### Model Download Issues

If automatic download fails, manually download from:
- YOLOv8 models: https://github.com/ultralytics/assets/releases

Place the `.pt` file in the camera_pipeline directory.

### Low FPS / Performance Issues

**Solutions:**
1. Use smaller model: `Config.FACE_MODEL_NAME = "yolov8n.pt"`
2. Reduce resolution: `Config.CAMERA_WIDTH = 640; Config.CAMERA_HEIGHT = 480`
3. Increase confidence threshold: `Config.FACE_CONFIDENCE_THRESHOLD = 0.6`
4. Enable frame skipping: `Config.FRAME_SKIP = 1`

### CUDA/GPU Support

For faster inference with NVIDIA GPU:

```bash
# Install PyTorch with CUDA support
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

YOLOv8 will automatically use GPU if available.

## Module Documentation

### CameraPipeline
Base pipeline class that orchestrates camera capture and display.

**Key Methods:**
- `initialize()`: Setup camera and display
- `run()`: Start pipeline loop
- `process_frame(frame)`: Override for custom processing
- `cleanup()`: Release all resources

### YOLOv8Detector
Object detection using YOLOv8 models.

**Key Methods:**
- `load_model()`: Initialize YOLOv8 model
- `detect_objects(frame, target_classes)`: Run detection
- `draw_boxes(frame, detections)`: Draw bounding boxes
- `process_frame(frame)`: Detect and draw in one step

### CameraCapture
Handles camera initialization, frame capture, and resource management.

**Key Methods:**
- `start()`: Initialize and start camera
- `read_frame()`: Read a single frame
- `stop()`: Release camera resources
- `get_properties()`: Get camera properties

### DisplayWindow
Manages display window and frame rendering.

**Key Methods:**
- `create_window()`: Create display window
- `show_frame(frame, fps)`: Display a frame
- `check_exit_key()`: Check for exit key press
- `destroy()`: Close window

## COCO Dataset Classes

The YOLOv8 models detect 80 object classes from the COCO dataset:

| Class ID | Object | Class ID | Object |
|----------|--------|----------|--------|
| 0 | person | 16 | dog |
| 1 | bicycle | 17 | cat |
| 2 | car | 24 | backpack |
| 3 | motorcycle | 26 | handbag |
| 5 | bus | 39 | bottle |
| 7 | truck | 41 | cup |
| 15 | bird | 67 | cell phone |

Full list: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml

## Performance Tips

1. **Model Selection**: 
   - `yolov8n` (~6 FPS on CPU, most real-time)
   - `yolov8s` (~4 FPS on CPU, balanced)
   - `yolov8m/l/x` (GPU recommended)

2. **Resolution**: Lower resolution = faster processing
   - 640x480: Best for CPU
   - 1280x720: Recommended for GPU
   - 1920x1080: High-end GPU only

3. **Confidence Threshold**: Higher = faster (fewer boxes to draw)

## Requirements

- Python 3.8+
- OpenCV 4.8.0+
- NumPy 1.24.0+
- Ultralytics 8.0.0+
- PyTorch 2.0.0+
- Webcam or camera device

## License

MIT License

## Support

For issues or questions:
1. Check the Troubleshooting section
2. Review module documentation
3. Consult ultralytics YOLOv8 docs: https://docs.ultralytics.com
